# 简介

在web服务器中，高可用是指服务器可以正常访问的时间，衡量的标准是在多长时间内可以提供正常服务。但是在Redis语境中，高可用的含义似乎要宽泛一些，除了保证提供正常服务( 如主从分离、快速容灾技术)，还需要考虑数据容量的扩展、数据安全不会丢失等。

在Redis中，实现高可用的技术主要包括持久化、主从复制、哨兵和cluster集群：

* 持久化： 持久化是最简单的高可用方法（有时甚至不被归为高可用的手段），主要作用是数据备份，即将数据存储在硬盘，保证数据不会因进程退出而丢失。
* 主从复制： 主从复制是高可用Redis的基础，哨兵和集群都是在主从复制基础上实现高可用的。主从复制主要实现了数据的多机备份（和同步），以及对于读操作的负载均衡和简单的故障恢复。缺陷：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制。
* 哨兵： 在主从复制的基础上，哨兵实现了自动化的故障恢复。（主挂了，找一个从成为新的主，哨兵节点进行监控）缺陷：写操作无法负载均衡；存储能力受到单机的限制。
* Cluster集群： 通过集群，Redis解决了写操作无法负载均衡，以及存储能力受到单机限制的问题，实现了较为完善的高可用方案。（6台起步，3主3从）

# 持久化

Redis支持RDB和AOF两种持久化机制，持久化功能有效地避免因进程退出造成的数据丢失问题，当下次重启时利用之前持久化的文件即可实现数据恢复。

## RDB

RDB(Redis Database)持久化是当前进程数据生成快照保存到硬盘的过程，触发RDB持久化过程分为手动触发和自动触发

### 触发机制

可以使用save和bgsave命令设置手动触发

* save命令：阻塞当前Redis服务器，直到RDB过程完成。当Redis实例内存很大时，会造成长时间阻塞，线上环境不建议使用
* bgsave命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生再fork阶段，一般时间很短。

除了手动触发外，Redis内部还存在自动触发RDB持久化的机制，如下场景会自动触发bgsave：

* 配置文件中使用`save`配置，如`save m n`表示m秒内数据集存在n次修改时
* 如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点
* 执行debug reload命令重新加载Redis时
* 默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则自动执行bgsave

### bgsave流程

bgsave是主流的触发RDB持久化方式，它的工作流程如下：

* 执行bgsave命令，Redis父进程判断当前是否存在正在执行的子进程，如RDB/AOF子进程，如果存在bgsave命令直接返回

* 父进程执行fork操作创建子进程，fork操作过程中父进程会阻塞；通过info stats命令查看latest_fork_usec选项，可以获取最近一个fork操作的时，单位为微秒 
* 父进程fork完成后，bgsave命令返回“Background saving started”信息并不再阻塞父进程，可以继续响应其他命令
* 子进程创建RDB文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换 ；执行lastsave命令可以获取最后一次生成RDB的时间，对应info统计的rdb_last_save_time选项  
* 进程发送信号给父进程表示完成，父进程更新统计信息，具体见info Persistence下的rdb_*相关选项  

### RDB文件的处理

RDB文件保存在`dir`配置指定的目录下，默认为`./`，文件名通过`dbfilename`配置指定

可以通过执行`config set dir newDir`和`config set dbfilename fileName`运行时动态更改配置。当遇到坏盘或者磁盘写满等情况时，可以通过上述命令修改文件路径到可用的磁盘路径。

Redis默认采用LZF算法对生成的RDB文件做压缩处理，压缩后的文件远远小于内存大小，默认开启。可以通过`rdbcompression {yes|no}`配置，也可以通过`config`命令动态修改。

虽然压缩RDB会消耗CPU，但是可以大幅降低文件的体积，方便保存到硬盘和通过网络发送到从节点，因此线上建议开启。

### 优缺点

优点：

* RDB是紧凑压缩的二进制文件，是Redis某个时间点上的数据快照，非常适合用于进行备份、灾备和全量复制的场景。
* Redis加载RDB回复数据远快于AOF的方式

* 是由子进程来处理生成RDB文件的工作的，主进程不进行任何的IO操作，不会影响redis的读写性能

缺点：

* 没有办法做到实时持久化，因为bgsave每次运行fock操作创建子进程属于重量级操作，不能频繁执行；所以最后一次持久化后的数据可能丢失，有一定的数据安全风险
* RDB使用特定二进制格式保存，新老版本Redis之间可能不兼容

## AOF

(Append-only File）AOF持久化：以独立日志的形式来记录每次写命令，重启时再重新执行AOF文件中的命令以达到恢复数据的目的。AOFn能够实时进行数据持久化，是Redis持久化的主流方式

### AOF持久化过程

开启AOF需要设置配置`appendonly yes`，默认不开启，AOF文件名通过`appendfilename`配置设置，默认文件名为`appendonly.aof`。保存路径也是通过`dir`配置指定。

AOF的工作流程如下:

* 客户端的请求写命令会被append追加到aof_buf缓冲区内
* AOF缓冲区根据AOF持久化策略将操作sync同步到磁盘的AOF文件中
* AOF文件大小超过重写策略或手动重写时，会对AOF文件rewrite重写，压缩AOF文件容量
* Redis服务重启时，会重新load加载AOF文件中的写操作达到数据恢复的目的

如果直接将命令写入硬盘，那么整个流程性能完全取决于硬盘性能。所以Redis先将命令写入缓冲区，之后再根据策略同步硬盘，并且Redis提供了多种同步硬盘的策略，再性能和安全方面做出平衡。

### 文件同步策略

在介绍策略之前，我们需要了解Liunx内核关于硬盘写的操作：

Liunx在内核提供页缓冲区来提高硬盘IO性能，系统调用write操作只将数据写入系统缓冲区后就直接返回，不管数据是否真的写入硬盘。而系统页缓冲区的同步策略依赖于系统调度机制，例如：缓冲区页空间写满或者达到特定时间周期。

而fsync针对单个文件操作做强制硬盘同步，fsync将阻塞直到写入硬盘后返回，确保了数据的持久化。

Redis提供了多种缓冲区同步文件的策略，由`appendfsync`配置控制：

* `always`命令写入aof_buf缓冲区后调用系统fsync操作进行同步。这样硬盘IO会成为写操作的性能瓶颈，不建议操作。
* `everysec`命令写入aof_buf后调用系统write操作。fsync的同步操作由专门线程每秒操作一次。是建议的同步策略，也是默认配置，兼顾了性能和安全性。理论上最多丢失1s的数据。
* `no`：命令写入aof_buf后调用系统write操作，不主动调用fsync操作，同步硬盘的步骤由操作系统来完成，通常同步周期最长为30s。虽然提高了性能，但是数据安全性完全无法保证。

### 重写机制

随着命令不断写入AOF，文件会越来越大，为了解决这个问题，Redis引入AOF重写机制压缩文件体积。AOF文件重写是把Redis进程内的数据转化为写命令同步到新AOF文件的过程。

重写后的AOF文件可以变小的原因有：

* 进程内已经超时的数据不再写入文件
* 旧的AOF文件含有无效命令，如`del`、`hdel`、`srem`等删除命令。重写使用进程内数据直接生成，这样新的AOF文件只保留最终数据的写入命令
* 多条写命令可以合并为一个，例如对列表的多次插入，可以合并为一条命令。

更小的的AOF文件不止能节省空间，还可以更快地被Redis加载。

AOF重写过程可以手动触发和自动触发：

* 手动触发，直接调用`bgrewriteaof`命令

* 当满足下面任意条件时，自动触发AOF重写：

  * aof_current_size >=auto-aof-rewrite-min-size

  * aof_base_size/aof_current_size>=auto-aof-rewrite-percentage

  其中auto-aof-rewrite-min-size和auto-aof-rewrite-percentage是配置参数，aof_current_size是当前AOF文件大小，aof_base_size是上一次重写后AOF文件的大小，可以info Persistence统计信息中查看

当触发AOF重写时，进行了一下流程：

* 执行AOF重写请求
  * 如果当前进程正在进行执行AOF重写，则请求不执行
  * 如果当前进程正在执行bgsave，重写命令延迟到bgsave完成之后执行
* 父进程执行fork创建子进程
* 主进程fork完成后，继续响应其他命令。所有修改命令仍然写入AOF缓冲区并根据策略同步到硬盘，保证原有的AOF机制正确执行

* 由于fork操作运用写时复制技术，子进程只能共享fork操作时的内存数据。由于父进程依然响应命令，Redis使用“AOF重写缓冲区”保存这部分新数据，防止新AOF文件生成期间丢失这部分数据  
* 子进程根据内存快照，按照命令合并规则写入到新的AOF文件。每次批量写入硬盘数据量由配置aof-rewrite-incremental-fsync控制，默认为32MB，防止单次刷盘数据过多造成硬盘阻塞
* 新AOF文件写入完成后，子进程发送信号给父进程，父进程更新统计信息，具体见info persistence下的aof_*相关统计
* 父进程把AOF重写缓冲区的数据写入到新的AOF文件
* 使用新AOF文件替换老文件，完成AOF重写

## 重启加载

AOF和RDB都可以用于重启后的数据回复，下面流程表示Redis持久化文件加载流程：

![Redis加载持久化文件流程](https://gitee.com/wangziming707/note-pic/raw/master/img/Redis%E5%8A%A0%E8%BD%BD%E6%8C%81%E4%B9%85%E5%8C%96%E6%96%87%E4%BB%B6%E6%B5%81%E7%A8%8B.png)

可以看到，AOF持久化开启并且存在AOF文件时，优先加载AOF文件。否则才会加载RDB文件。

## 问题定位和优化

Redis持久化功能一直是影响Redis性能的高发地，我们介绍常见的持久化问题进行分析定位和优化

### fork操作

当Redis做RDB或AOF重写时，一个必不可少的操作就是执行fork操作创建子进程，对于大多数操作系统来说fork是个重量级错误。虽然fork创建的子进程不需要拷贝父进程的物理内存空间，但是会复制父进程的空间内存页表。例如对于10GB的Redis进程，需要复制大约20MB的内存页表，因此fork操作耗时跟进程总内存量息息相关  

**问题定位**:对于高流量的Redis实例OPS可达5万以上，如果fork操作耗时在秒级别将拖慢Redis几万条命令执行，对线上应用延迟影响非常明显正常情况下fork耗时应该是每GB消耗20毫秒左右。可以在info stats统计中查latest_fork_usec指标获取最近一次fork操作耗时，单位微秒   

fork操作耗时优化：

* 优先使用物理机或者高效支持fork操作的虚拟化技术，避免使用Xen

* 控制Redis实例最大可用内存，fork耗时跟内存量成正比，线上建议每个Redis实例内存控制在10GB以内
* 合理配置Linux内存分配策略，避免物理内存不足导致fork失败
* 降低fork操作的频率，如适度放宽AOF自动触发时机，避免不必要的全量复制等

### 子进程开销

子进程负责AOF或者RDB文件的重写，它的运行过程主要涉及CPU、 内存、 硬盘三部分的消耗

#### CPU

子进程负责把进程内的数据分批写入文件，这个过程属于CPU密集操作，通常子进程对单核CPU利用率接近90%

优化：

* Redis是CPU密集型服务，不要做绑定单核CPU操作。由于子进程非常消耗CPU，会和父进程产生单核资源竞争
* 不要和其他CPU密集型服务部署在一起，造成CPU过度竞争
* 如果部署多个Redis实例，尽量保证同一时刻只有一个子进程执行重写工作

#### 内存

子进程通过fork操作产生，占用内存大小等同于父进程，理论上需要两倍的内存来完成持久化操作，但Linux有写时复制机制（copy-on-write） 。父子进程会共享相同的物理内存页，当父进程处理写请求时会把要修改的页创建副本，而子进程在fork操作过程中共享整个父进程内存快照。

在RDB和AOF重写时，Redis日志会输出相关内存占用信息。

优化：

* 同CPU优化一样，如果部署多个Redis实例，尽量保证同一时刻只有一个子进程在工作

* 避免在大量写入时做子进程重写操作，这样将导致父进程维护大量页副本，造成内存消耗

#### 硬盘

子进程主要职责是把AOF或者RDB文件写入硬盘持久化。势必造成硬盘写入压力

优化：

* 不要和其他高硬盘负载的服务部署在一起。如： 存储服务、 消息队列服务等
* AOF重写时会消耗大量硬盘IO，可以开启配置no-appendfsync-onrewrite，默认闭。表示在AOF重写期间不做fsync操作
* 当开启AOF功能的Redis用于高流量写入场景时，如果使用普通机械磁盘，写入吞吐一般在100MB/s左右，这时Redis实例的瓶颈主要在AOF同步硬盘上
* 对于单机配置多个Redis实例的情况，可以配置不同实例分盘存储AOF文件，分摊硬盘写入压力

### AOF追加阻塞

当开启AOF持久化时，常用的同步硬盘的策略是everysec，用于平衡性能和数据安全性。对于这种方式，Redis使用另一条线程每秒执行fsync同步硬盘。当系统硬盘资源繁忙时，会造成Redis主线程阻塞。

当同步线程同步磁盘因为硬盘IO繁忙阻塞时，主线程会对比上次fsync时间，如果大于2s，那么主线程会阻塞等待fsync操作完成才会继续处理命令。图示如下：

![AOF追加阻塞示意图](https://gitee.com/wangziming707/note-pic/raw/master/img/AOF%E8%BF%BD%E5%8A%A0%E9%98%BB%E5%A1%9E%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

所以我们发现：

* everysec配置最多可能丢失2秒数据，不是1秒
* 如果系统fsync缓慢，将会导致Redis主线程阻塞影响效率

当发生这样的AOF阻塞时，Redis会输出一下日志：

~~~log
Asynchronous AOF fsync is taking too long (disk is busy). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis
~~~

每当发生AOF追加阻塞事件发生时，在info Persistence统计中，aof_delayed_fsync指标会累加，查看这个指标方便定位AOF阻塞问题  

# 复制

在分布式系统中为了解决单点故障，通常会把数据复制多个副本部署到其他及其，满足故障恢复和负载均衡等要求。Redis也是如厕，它为我们提供了复制功能，实现了相同数据的多个Redis副本。复制功能是高可用Redis的基础。

## 配置

### 建立复制

参与复制的Redis实例划分为主节点(master)和从节点(slave)。默认情况下，Redis都是主节点。每个从节点只能有一个主节点，而主节点可以有多个从节点。复制的数据流是单向的，只能由主节点复制到从节点。

将当前Redis实例设置成从节点有如下三种方式：

* 配置文件中：设置`slaveof <master-ip> <master-port>`。随Redis启动生效
* 在redis-server启动命令后加上`--slaveof <master-ip> <master-port>`生效
* 客户端直接使用命令：`slaveof <master-ip> <master-port> `

建立主从结构后可以通过`info replication`命令查看role信息确认当前节点的角色。

### 断开复制

在从节点上执行`slaveof no one`断开于主节点的复制关系。从节点晋升为主节点。

从节点断开复制后不会抛弃原有数据，只是无法再获取主节点上的数据变化

通过slaveof命令还可以实现切主操作，所谓切主是指把当前从节点对主节点的复制切换到另一个主节点。执行`slaveof{newMasterIp}{newMasterPort}`命令即可  

切换主节点后从节点会清空之前所有数据，需要小心操作。

### 安全性

对于数据比较重要的节点，主节点会通过设置`requirepass`参数进行密码验证，这时所有的客户端访问必须使用auth命令进行校验。从节点和主节点的复制连接是通过一个特殊标识的客户端来完成的，因此需要配置从节点的`masterauth`参数于主节点密码保持一致，这样从节点才能正确地连接到主节点。

### 只读

默认情况下，从节点使用`slave-read-only=yes`配置为只读模式。由于复制只能是主节点到从节点，如果从节点有任何改动，主节点无法感知同步，会造成主从不一致，所以建议不要修改改配置。

### 传输延迟

主从节点一般部署在不同的机器上，复制时的网络延迟是需要考虑的问题。Redis提供了`repl-disable-tcp-nodelay`参数用于控制是否关闭tcp_nodelay，默认关闭，说明如下：

* 当关闭时，主节点产生的命令数据无论大小都会及时发送给从节点，这样主从之间的延迟会变小，但增加了网络带宽的消耗。适用于主从之间网络环境良好、或者同机架同机房部署的场景。
* 当开启时，主节点会合并比较小的TCP数据包从而节省带宽。默认发送时间间隔取决于Liunx的内核，一般默认为40毫秒。这种配置节省了带宽但增大主从之间的延迟。适用于主从网络环境复杂或者带宽紧张的场景。

## 拓扑

Redis的复制拓扑结构可以支持单层或者多层复制关系

### 一主一从

一主一从结构是最简单的复制拓扑结构，用于主节点出现宕机时从节点提供故障转移支持当应用写命令并发量较高且需要持久化时，可以只在从节点上开启AOF，这样既保证数据安全性同时也避免了持久化对主节点的性能干扰。但需要注意的是，当主节点关闭持久化功能时，如果主节点脱机要避免自动重启操作。因为主节点之前没有开启持久化功能自动重启后数据集为空，这时从节点如果继续复制主节点会导致从节点数据也被清空的情况，丧失了持久化的意义。安全的做法是在从节点上执行slaveof no one断开与主节点的复制关系，再重启主节点从而避免这一问题。

### 一主多从

一主多从结构使得应用端可以利用多个从节点实现读写分离。

对于读占比较大的场景，可以把读命令发送到从节点来分担主节点压力。同时在日常开发中如果需要执行一些比较耗时的读命令，如： keys、 sort等，可以在其中一台从节点上执行，防止慢查询对主节点造成阻塞从而影响线上服务的稳定性。

对于写并发量较高的场景，多个从节点会导致主节点写命令的多次发送从而过度消耗网络带宽，同时也加重了主节点的负载影响服务稳定性。

### 树状主从结构

树状主从结构使得从节点不但可以复制主节点数据，同时可以作为其他从节点的主节点继续向下层复制。

通过引入复制中间层，可以有效降低主节点负载和需要传送给从节点的数据量。

在这样的结构中数据实现了一层一层的向下复制。当主节点需要挂载多个从节点时为了避免对主节点的性能干扰，可以采用树状主从结构降低主节点压力。

## 原理

### 复制过程

从节点执行slaveof命令后，复制过程便开始运作，下面是完整流程：

* 保存主节点信息，在建立连接前，首先将主节点的host和port信息保存

* 从节点(slave)内部通过每秒运行的定时任务维护复制相关逻辑，当定时任务发现存在新的主节点后，会尝试与该节点建立网络连接：从节点会建立一个socket套接字，专门用于接收主节点发送的复制命令。

  如果从节点无法建立连接，定时任务会无限重试直到连接成功或者执行`slaveof no one`取消复制。可以通过info replication查看master_link_down_since_seconds指标  ，它记录与主节点连接失败的系统时间。从节点连接主节点失败时也会每秒打印日志。

* 连接建立成功后从节点发送ping请求进行首次通信，ping请求主要目的如下：

  * 检测主从之间socket是否可用
  * 检测主节点当前是否可以接收处理命令

  如果发送ping命令后，从节点没有收到主节点pong回复或者超时，从节点会断开复制连接，下次定时任务会发起重连。

* 权限验证。如果主节点设置了requirepass参数，从节点必须配置masterauth参数保证与主节点相同的密码才能通过验证，如果验证失败复制将终止。

* 同步数据集。主从复制连接正常通信后，对于首次建立复制的场景，主节点会把持有的数据全部发送给从节点，这部分操作时耗时很长，将异步执行

* 命令持续复制。当主节点把当前数据同步给从节点后，便完成了复制的建立流程。接下来主节点会持续地把写命令发送给从节点，保证主从数据一致性

### 数据同步

Redis使用psync命令完成主从数据同步，同步分为全量复制和部分复制

* 全量复制：一般用于初次复制场景，它会把主节点全部数据一次性发送给从节点，当数据量较大时，会对主从节点和网络造成很大的开销
* 部分复制：用于处理在主从复制中因网络闪断等原因造成的数据丢失场景，当从节点再次连上主节点后，如果条件允许，主节点会补发丢失数据给从节点。因为补发的数据远远小于全量数据，可以有效避免全量复制的过高开销

psync命令需要以下组件支持：

* 主从节点各自复制偏移量
* 主节点复制积压缓冲区
* 主节点运行id

#### 复制偏移量

参与复制的主从节点都会维护自身复制偏移量，主节点（master） 在处理完写入命令后，会把命令的字节长度做累加记录，统计信息在info relication中的master_repl_offset指标中。

从节点每秒上报自身的复制偏移量给主节点，因此主节点也会保存从节点的复制偏移量，可以在`info relication`中查看

从节点在接收到主节点发送的命令后，也会累加记录自身的偏移量。统计信息在info relication中的slave_repl_offset指标中。

#### 复制积压缓冲区

复制积压缓冲区是保存在主节点上的一个固定长度的队列， 默认大小为1MB

当主节点有连接的从节点时被创建， 这时主节点响应写命令时， 不但会把命令发送给从节点， 还会写入复制积压缓冲区

由于缓冲区本质上是先进先出的定长队列， 所以能实现保存最近已复制数据的功能， 用于部分复制和复制命令丢失的数据补救。

复制缓冲区相关统计信息保存在主节点的`info replication`中：

~~~bat
127.0.0.1:6379> info replication
# Replication
role:master
...
repl_backlog_active:1 // 开启复制缓冲区
repl_backlog_size:1048576 // 缓冲区最大长度
repl_backlog_first_byte_offset:7479 // 起始偏移量， 计算当前缓冲区可用范围
repl_backlog_histlen:1048576 // 已保存数据的有效长度
~~~

#### 主节点运行ID

每个Redis节点启动后都会动态分配一个40位的16进制字符串作为运行ID。运行ID的主要作用是用来唯一识别Redis节点， 比如从节点保存主节点的运行ID识别自己正在复制的是哪个主节点。

如果只使用ip+port的方式识别主节点， 那么主节点重启变更了整体数据集（如替换RDB/AOF文件）从节点再基于偏移量复制数据将是不安全的， 因此当运行ID变化后从节点将做全量复制

可以运行info server命令查看当前节点的运行ID。需要注意的是Redis关闭再启动后， 运行ID会随之改变

可以使用debug reload命令重新加载RDB并保持运行ID不变，这在做一些需要重启redis操作时会很有用，如调优一些内存相关配置，需要Redis重新加载才能优化已存在的数据

**注意：**debug reload命令会阻塞当前Redis节点主线程， 阻塞期间会生成本地RDB快照并清空数据之后再加载RDB文件。 因此对于大数据量的主节点和无法容忍阻塞的应用场景， 谨慎使用

#### psync

从节点使用psync命令完成部分复制和全量复制，命令格式为：`psync {runId} {offset}`，参数含义如下：

* runId:从节点所复制主节点的运行ID
* offset：当前从节点已复制的数据偏移量

psync命令，的整体流程如下：

* 从节点发送psync命令给主节点
  * 参数runId时当前从节点保存的主节点运行ID，如果没有则默认值为？
  * 参数offset时当前从节点保存的复制偏移量，如果是第一次参与复制则默认值为-1
* 主节点根据psync参数与自身数据情况决定响应结果：
  * 回复`+FULLRESYNC {runId} {offset}`，那从节点将触发全量复制流程
  * 回复`+CONTINUE`，从节点将触发部分复制流程
  * 回复`+ERR`说明主节点版本过低无法识别psync命令，从节点将发送旧版的sync命令触发全量复制流程

### 全量复制

全量复制是主从第一次建立复制时必须经历的阶段。通过psync发起全量复制，流程如下：

* 发送psync命令进行数据同步，第一次复制时发送`psync -1`

* 主节点根据psync-1解析当前为全量复制，回复`+FULLRESYNC`响应

* 从节点接收主节点的响应数据保存运行ID和偏移量offset

* 主节点执行bgsave保存RDB文件到本地

* 主节点发送RDB文件给从节点，从节点把接收的RDB文件保存在本地并直接作为从节点的数据文件。

  如果数据量比较打，可能耗时会很久，当耗时超过`repl-timeout`（默认60s）时全量复制会失败。所以此时可以调高`repl-timeout`的值

  * **无盘复制**：为了降低主节点的磁盘开销，Redis支持无盘复制，生成的RDB文件不保存到硬盘而是直接通过网络发送给从节点，通过`repl-diskless-sync`参数控制，默认关闭。无盘复制适用于主节点磁盘性能较差但网络带宽充裕的场景。

* 从节点开始接收RDB快照到接收完成期间，主节点仍然响应读写命令，因此主节点会把这期间写命令数据保存在复制客户端缓冲区内，当从节点接在完RDB文件后，主节点再把缓冲区内的数据发送给从节点，从而保证主从一致。如果传输RDB的时间过长，对于高流量写入场景容易造成主节点复制客户端缓冲区溢出。

  默认配置为`clientoutput-buffer-limit slave256MB64MB60`， 如果60秒内缓冲区消耗持续大于64MB或者直接超过256MB时， 主节点将直接关闭复制客户端连接， 造成全量同步失败.所以要根据情况配置该值。

* 从节点接收完主节点传送过来的全部数据后会清空自身旧数据

* 从节点清空完成后开始加载RDB文件，当文件较大时，这一步骤比较耗时

  在做读写分离的场景下，此时从节点仍然可以响应读命令。但是因为正处于全量复制货主额复制终端阶段，从节点响应读命令可能拿到过期或错误的数据。Redis提供了`slave-serve-stale-data `参数，设置为no时，将不能响应除了info和slaveof之外的所有命令。如果对一致性要求很高，可以在复制期间关闭该参数。

* 从节点成功加载完RDB后，如果从节点开启了AOF，它会立刻作bgrewriteaof进行aof重写。

可以发现全量复制在某些步骤非常耗时，如主节点bgsave、RDB文件网络传输、从节点加载RDB、AOF重写等。

所以除了第一次复制时采用全量复制外，其他场景都应该规避全量复制的发生。

### 部分复制

部分复制主要是Redis针对全量复制的过高开销做出的一种优化措施，使用`psync{runId}{offset}`命令实现。 当从节点正在复制主节点时， 如果出现网络闪断或者命令丢失等异常情况时， 从节点会向主节点要求补发丢失的命令数据， 如果主节点的复制积压缓冲区内存在这部分数据则直接发送给从节点， 这样就可以保持主从节点复制的一致性。 补发的这部分数据一般远远小于全量数据， 所以开销很小。  

流程说明：

* 当主从节点之间网络出现中断时， 如果超过repl-timeout时间 主节点会认为从节点故障并中断复制连接。
* 主从连接中断期间主节点依然响应命令， 但因复制连接中断命令无法发送给从节点， 不过主节点内部存在的复制积压缓冲区， 依然可以保存最近一段时间的写命令数据， 默认最大缓存1MB
* 当主从连接恢复后， 由于从节点之前保存了自身已复制的偏移量和主节点的运行ID。 因此会把它们当作psync参数发送给主节点， 要求进行部分复制操作
* 主节点接到psync命令后首先核对参数runId是否与自身一致， 如果一致， 说明之前复制的是当前主节点； 之后根据参数offset在自身复制积压缓冲区查找， 如果偏移量之后的数据存在缓冲区中，则对从节点发送`+CONTINUE`响应， 表示可以进行部分复制  
* 主节点根据偏移量把复制积压缓冲区里的数据发送给从节点， 保证主从复制进入正常状态。

### 心跳

主从节点建立复制后，它们之间维护长连接并彼此发送心跳命令。其判断机制如下：

* 主从节点之间都有心跳检测机制，各自模拟成对方的客户端进行通信

  通过`client list`查看客户端信息，主节点模拟的客户端`flags=M`，从节点模拟客户端的`flags=S`

* 主节点默认每隔10秒对从节点发送ping命令，判断从节点的存活性和连接状态，可通过`repl-ping-slave-period`控制发送频率

* 从节点在主线程中每隔1秒发送`replconf ack {offset}`命令，给主节点上报当前自生的复制偏移量，该作用有：

  * 实时检测主从节点网络状态
  * 上报自生复制偏移量，检查复制数据是否丢失
  * 保证从节点的数量和延迟性功能

### 异步复制

主节点不但负责数据读写，还负责把命令同步给从节点。写命令的发送过程是异步完成的。也就是说主节点处理完写命令后立即返回给客户端，不等待从节点复制完成。

主节点复制过程：

* 主节点接收处理命令
* 命令处理完成后立即返回响应结果
* 对于修改命令异步发送给从节点，从节点在主线程中执行复制的命令

因为主从复制过程是异步的，所以会造成从节点的数据相对主节点存在延迟。可以通过`info replication`命令查看相关主从节点的offset判断。主从节点的offset差值就是它们延迟量。

# 哨兵













# Temp

## Sentinel

sentinel哨兵提供主从复制模式的故障转移的的自动化处理

### 原理

Redis Sentinel是运行在特殊模式下的Redis服务器

有三个主要任务：

* 监控：Sentinel不断检查主服务器和从服务器是否按照预期正常工作，

  每隔固定时间sentinel会请求主服务器，如果没有收到主服务的正常响应，则报告异常

* 提醒：被监视的Redis出现问题时，Sentinel会通知管理员或其他应用程序

* 自动故障转移：监控的主Redis不能正常工作，Sentinel会开始进行故障迁移操作。

  将一个从服务器升级新的主服务器。让其他从服务器挂到新的主服务器。同时向客户端提供新的主服务器地址

Sentinel分布式系统：

* 如果只有一个Sentinel，那么Sentinel出现问题就无法监控。所以需要多个哨兵，组成Sentinel网络。一个健康的sentinel至少有三个Sentinel应用。彼此在独立的物理机器或虚拟机
* 监控同一个Master的Sentinel会自动连接，组成一个分布式的网络，互相通信并彼此交换关于被监控服务器的信息
* 当一个 Sentinel 认为被监控的服务器已经下线时，它会向网络中的其它 Sentinel 进行确认，判断该服务器是否真的已经下线 
* 如果下线的服务器为主服务器，那么 Sentinel 网络将对下线主服务器进行自动故障转移，通过将下线主服务器的某个从服务器提升为新的主服务器，并让其从服务器转移到新的主服务器下，以此来让系统重新回到正常状态 
* 下线的旧主服务器重新上线，Sentinel 会让它成为从，挂到新的主服务器下 

### 实现

配置sentinel.conf文件：

~~~python
#sentinel自己的接口:
port <port>
#sentinel要监视的master:
sentinel monitor <name> <masterIP> <masterPort> <Quorum>
# quorum 表示投票数，当有超过该数量的哨兵认为服务器已经下线，才会判断下线
~~~

启动sentinel服务器：

~~~python
redis-sentinel sentinel.conf
~~~

## Cluster

Sentinel 集群方案中只有一个主服务器，只提高了redis 的可用性，并没有提高redis 的性能，如果业务对redis性能有高要求，需要搭建多台主服务器，来提高redis性能，这就是cluster集群

为了实现高可用，最好一个集群有3个master节点，每个master节点至少有1个子节

哈希槽：

cluster集群的设计是去中性化的

它引入了哈希槽的概念，Redis集群有16384个哈希槽

在集群中的每个主节点会被分配相等个数的哈希槽

在进行set操作时，每个key会通过 CRC16 算法得出当前key对应的哈希槽，这样就能知道这个key应该去往的master节点

gossip协议：

redis集权通过gossip协议进行通讯，保证所有节点都会知道整个集群完整的信息
